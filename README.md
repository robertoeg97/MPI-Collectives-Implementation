# MPI Collectives Implementation

Due to academic integrity, the project source code and detailed report are private. This public summary outlines the core objectives, challenges, and achievements. Code/reports cannot be shared, even on request.

## Project Overview

This project involved designing and implementing several fundamental collective communication operations using the Message Passing Interface (MPI) standard. These collectives are essential building blocks for scalable parallel applications in high-performance computing (HPC) environments.

**Key Objectives:**
- Deepen understanding of distributed memory parallelism and collective communication patterns.
- Gain hands-on experience with MPI, a widely used standard for parallel programming on clusters and supercomputers.
- Analyze and optimize the performance of collective operations with respect to latency and bandwidth.

## My Role & Contributions

- Implemented core MPI collective operations (Reduce, Scatter, Gather, Allgather, Broadcast) using only point-to-point communication primitives.
- Designed and tested tree-based and bucket-based algorithms for efficient data movement across processes.
- Developed and executed performance benchmarks to evaluate scalability and communication efficiency.
- Documented design decisions and performance trade-offs for each collective operation.

## Skills & Technologies

- **MPI (OpenMPI):** Practical experience with message passing, synchronization, and collective communication.
- **Parallel Algorithm Design:** Applied tree-based and bucket-based strategies to optimize communication.
- **Performance Analysis:** Used profiling tools and custom benchmarks to measure and improve collective operation efficiency.
- **C Programming:** Developed robust, portable code for distributed systems.

## Learning Outcomes

- Gained a strong foundation in parallel programming concepts and MPI communication patterns.
- Developed the ability to analyze and optimize distributed algorithms for real-world HPC workloads.
- Strengthened skills in debugging, profiling, and validating parallel applications.
